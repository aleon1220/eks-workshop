# EKS workshop session 01
aleon rangel
- https://www.eksworkshop.com/020_prerequisites/k8stools/#install-kubectl
sudo curl --silent --location -o /usr/local/bin/kubectl \
  https://amazon-eks.s3.us-west-2.amazonaws.com/1.17.7/2020-07-08/bin/linux/amd64/kubectl

sudo chmod +x /usr/local/bin/kubectl

Update awscli
Upgrade AWS CLI according to guidance in AWS documentation.
sudo pip install --upgrade awscli && hash -r


## install extra utilities
sudo yum -y install jq gettext bash-completion moreutils

## https://www.eksworkshop.com/020_prerequisites/k8stools/#install-yq-for-yaml-processing
echo 'yq() {
  docker run --rm -i -v "${PWD}":/workdir mikefarah/yq yq "$@"
}' | tee -a ~/.bashrc && source ~/.bashrc

https://www.eksworkshop.com/020_prerequisites/k8stools/#enable-kubectl-bash-completion

kubectl completion bash >>  ~/.bash_completion
. /etc/profile.d/bash_completion.sh
. ~/.bash_completion

## https://www.eksworkshop.com/020_prerequisites/k8stools/#set-the-aws-alb-ingress-controller-version
`echo 'export ALB_INGRESS_VERSION="v1.1.8"' >>  ~/.bash_profile
.  ~/.bash_profile`
- replace IAM role in EC2 that controls cloud9 eksworkshop-admin


## Set AWS region through CLI

```bash
export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)
export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')
```

## Save details to bash profile
```bash
echo "export ACCOUNT_ID=${ACCOUNT_ID}" | tee -a ~/.bash_profile
echo "export AWS_REGION=${AWS_REGION}" | tee -a ~/.bash_profile
aws configure set default.region ${AWS_REGION}
aws configure get default.region
```

### validate
andres-leonRangel:~/environment $ aws sts get-caller-identity --query Arn | grep eksworkshop-admin -q && echo "IAM role valid" || echo "IAM role NOT valid"
IAM role valid

## CLONE THE SERVICE REPOS
cd ~/environment
git clone https://github.com/brentley/ecsdemo-frontend.git
git clone https://github.com/brentley/ecsdemo-nodejs.git
git clone https://github.com/brentley/ecsdemo-crystal.git

# SSH Access
The key fingerprint is:
SHA256:qoGDZvt0XY09pUOXkmhgnXgPLBRr83V3p+/uQvrlEfE ec2-user@ip-172-31-24-0
The key's randomart image is:
+---[RSA 2048]----+
|      .== .      |
|      .oo*. . .  |
|       +ooo= = oo|
|      . + *.* ..=|
|        S+ *  ..E|
| . .  ...   o ...|
|.oo.....     o .o|
|o o..o      . .+.|
| ....        ..+=|
+----[SHA256]-----+

## Upload the public key to your EC2 region
aws ec2 import-key-pair --key-name "eksworkshop" --public-key-material file://~/.ssh/id_rsa.pub
{
    "KeyFingerprint": "a0:49:df:b8:03:d5:0b:bc:a4:ff:79:e6:e6:7f:ea:1b",
    "KeyName": "eksworkshop",
    "KeyPairId": "key-057b5a6a517da5e86"
}

## Create a CMK Custom Managed Key for the EKS cluster to use when encrypting your Kubernetes secrets:
aws kms create-alias --alias-name alias/eksworkshop --target-key-id $(aws kms create-key --query KeyMetadata.Arn --output text)


# PREREQUISITES
## Launch using EKSCTL
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp

sudo mv -v /tmp/eksctl /usr/local/bin

## EKS CTL bash completion
```bash
eksctl completion bash >> ~/.bash_completion
. /etc/profile.d/bash_completion.sh
. ~/.bash_completion
```

# Launch EKS Cluster
- check IAM
- create deployment file `eksworkshop.yaml`
- launch with
`eksctl create cluster -f eksworkshop.yaml`

``` bash
andres-leonRangel:~/environment $ eksctl create cluster -f eksworkshop.yaml
[ℹ]  eksctl version 0.29.1
[ℹ]  using region us-west-2
[ℹ]  subnets for us-west-2a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for us-west-2b - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for us-west-2c - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  using EC2 key pair "eksworkshop"
[ℹ]  using Kubernetes version 1.17
[ℹ]  creating EKS cluster "eksworkshop-eksctl" in "us-west-2" region with managed nodes
[ℹ]  1 nodegroup (nodegroup) was included (based on the include/exclude rules)
[ℹ]  will create a CloudFormation stack for cluster itself and 0 nodegroup stack(s)
[ℹ]  will create a CloudFormation stack for cluster itself and 1 managed nodegroup stack(s)
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=eksworkshop-eksctl'
[ℹ]  CloudWatch logging will not be enabled for cluster "eksworkshop-eksctl" in "us-west-2"
[ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-west-2 --cluster=eksworkshop-eksctl'
[ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "eksworkshop-eksctl" in "us-west-2"
[ℹ]  2 sequential tasks: { create cluster control plane "eksworkshop-eksctl", 2 sequential sub-tasks: { no tasks, create managed nodegroup "nodegroup" } }
[ℹ]  building cluster stack "eksctl-eksworkshop-eksctl-cluster"
[ℹ]  deploying stack "eksctl-eksworkshop-eksctl-cluster"


[ℹ]  building managed nodegroup stack "eksctl-eksworkshop-eksctl-nodegroup-nodegroup"
[ℹ]  deploying stack "eksctl-eksworkshop-eksctl-nodegroup-nodegroup"
[ℹ]  waiting for the control plane availability...
[✔]  saved kubeconfig as "/home/ec2-user/.kube/config"
[ℹ]  no tasks
[✔]  all EKS cluster resources for "eksworkshop-eksctl" have been created
[ℹ]  nodegroup "nodegroup" has 3 node(s)
[ℹ]  node "ip-192-168-1-20.us-west-2.compute.internal" is ready
[ℹ]  node "ip-192-168-48-149.us-west-2.compute.internal" is ready
[ℹ]  node "ip-192-168-95-54.us-west-2.compute.internal" is ready
[ℹ]  waiting for at least 3 node(s) to become ready in "nodegroup"
[ℹ]  nodegroup "nodegroup" has 3 node(s)
[ℹ]  node "ip-192-168-1-20.us-west-2.compute.internal" is ready
[ℹ]  node "ip-192-168-48-149.us-west-2.compute.internal" is ready
[ℹ]  node "ip-192-168-95-54.us-west-2.compute.internal" is ready
[ℹ]  kubectl command should work with "/home/ec2-user/.kube/config", try 'kubectl get nodes'
[✔]  EKS cluster "eksworkshop-eksctl" in "us-west-2" region is ready
```
# TEST THE CLUSTER
## kubectl get nodes # if we see our 3 nodes, we know we have authenticated correctly
```bash
STACK_NAME=$(eksctl get nodegroup --cluster eksworkshop-eksctl -o json | jq -r '.[].StackName')
ROLE_NAME=$(aws cloudformation describe-stack-resources --stack-name $STACK_NAME | jq -r '.StackResources[] | select(.ResourceType=="AWS::IAM::Role") | .PhysicalResourceId')
echo "export ROLE_NAME=${ROLE_NAME}" | tee -a ~/.bash_profile
```

# BEGINNER EKS workshop
## 1 Kubernetes Dashboard
```bash
export DASHBOARD_VERSION="v2.0.0"

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/${DASHBOARD_VERSION}/aio/deploy/recommended.yaml

## output
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/${DASHBOARD_VERSION}/aio/deploy/recommended.yaml
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
```

## EKS get token
`aws eks get-token --cluster-name eksworkshop-eksctl | jq -r '.status.token'`

## CLEANUP K8S Dashboard
`pkill -f 'kubectl proxy --port=8080'`

# delete dashboard
`kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/${DASHBOARD_VERSION}/aio/deploy/recommended.yaml`

unset DASHBOARD_VERSION

```bash
aws ec2 import-key-pair --key-name "eksworkshop" --public-key-material file://~/.ssh/id_rsa.pub
{
    "KeyFingerprint": "11:1a:bf:21:97:69:50:a1:fc:d7:08:5b:42:05:4a:9a",
    "KeyName": "eksworkshop",
    "KeyPairId": "key-02723750093f8b7da"
}
```

# AWS KMS CUSTOM MANAGED KEY (CMK)
Create a CMK for the EKS cluster to use when encrypting your Kubernetes secrets:
```bash
aws kms create-alias --alias-name alias/eksworkshop --target-key-id \
  $(aws kms create-key --query KeyMetadata.Arn --output text)
```

Let’s retrieve the ARN of the CMK to input into the create cluster command.
```bash
export MASTER_ARN=$(aws kms describe-key --key-id alias/eksworkshop --query KeyMetadata.Arn --output text)
```
Let’s retrieve the ARN of the CMK to input into the create cluster command.
```bash
export MASTER_ARN=$(aws kms describe-key --key-id alias/eksworkshop --query KeyMetadata.Arn --output text)
```

We set the MASTER_ARN environment variable to make it easier to refer to the KMS key later.
Now, let’s save the MASTER_ARN environment variable into the bash_profile

`echo "export MASTER_ARN=${MASTER_ARN}" | tee -a ~/.bash_profile`

# EKS commands
`kubectl get nodes`

## Export the Worker Role Name
```bash
STACK_NAME=$(eksctl get nodegroup --cluster eksworkshop-eksctl -o json | jq -r '.[].StackName')
ROLE_NAME=$(aws cloudformation describe-stack-resources --stack-name $STACK_NAME | jq -r '.StackResources[] | select(.ResourceType=="AWS::IAM::Role") | .PhysicalResourceId')
echo "export ROLE_NAME=${ROLE_NAME}" | tee -a ~/.bash_profile
```