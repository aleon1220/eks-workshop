# EKS workshop
## 2-DEPLOY THE EXAMPLE MICROSERVICES [AWS link](https://www.eksworkshop.com/beginner/050_deploy/)

`alias kproxy='kubectl proxy --port=8080 --address=0.0.0.0 --disable-filter=true &'`

- deployed all the microservices

# HELM

## https://www.eksworkshop.com/beginner/060_helm/helm_intro/install/#install-the-helm-cli
 Chart repositories are similar to APT or yum repositories that you might be familiar with on Linux, or Taps for Homebrew on macOS.

`helm search repo stable`

## Configure Bash completion for the helm command
```
helm completion bash >> ~/.bash_completion
. /etc/profile.d/bash_completion.sh
. ~/.bash_completion
source <(helm completion bash)
```

## Deploy nginx With Helm
### Update the Chart Repository
# first, add the default repository, then update
helm repo add stable https://kubernetes-charts.storage.googleapis.com/
helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "stable" chart repository
Update Complete. ⎈Happy Helming!⎈

### Search Chart Repositories
helm search repo nginx

### Add the Bitnami Repository
Bitnami Chart repository.
helm repo add bitnami https://charts.bitnami.com/bitnami

helm search repo bitnami

helm search repo bitnami/nginx

### Install bitnami/nginx
helm install --help

andres-leonRangel:~/environment/ecsdemo-nodejs (master) $ helm install bitnami-nginx bitnami/nginx
NAME: bitnami-nginx
LAST DEPLOYED: Wed Oct  7 01:43:17 2020
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
** Please be patient while the chart is being deployed **

NGINX can be accessed through the following DNS name from within your cluster:

    bitnami-nginx.default.svc.cluster.local (port 80)

To access NGINX from outside the cluster, follow the steps below:

1. Get the NGINX URL by running these commands:

  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
        Watch the status with: 'kubectl get svc --namespace default -w bitnami-nginx'

    export SERVICE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].port}" services bitnami-nginx)
    export SERVICE_IP=$(kubectl get svc --namespace default bitnami-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
    echo "http://${SERVICE_IP}:${SERVICE_PORT}"

`kubectl get svc,po,deploy`
kubectl get service bitnami-nginx -o wide
af90547616f504d8a9ce5086138b9036-1052822087.us-west-2.elb.amazonaws.com
### Clean Up

To remove all the objects that the Helm Chart created, we can use Helm uninstall.

# Deploy example microservices with HELM
### Helm structure
```
/eksdemo
  /Chart.yaml  # a description of the chart
  /values.yaml # defaults, may be overridden during install or upgrade
  /charts/ # May contain subcharts
  /templates/ # the template files themselves
  ...
```

## delete boiler plate
```
rm -rf ~/environment/eksdemo/templates/
rm ~/environment/eksdemo/Chart.yaml
rm ~/environment/eksdemo/values.yaml
```

## copy the manifest files for each of our microservices into the templates directory as servicename.yaml
#create subfolders for each template type
mkdir -p ~/environment/eksdemo/templates/deployment
mkdir -p ~/environment/eksdemo/templates/service

# Copy and rename frontend manifests
cp ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/frontend.yaml
cp ~/environment/ecsdemo-frontend/kubernetes/service.yaml ~/environment/eksdemo/templates/service/frontend.yaml

# Copy and rename crystal manifests
cp ~/environment/ecsdemo-crystal/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/crystal.yaml
cp ~/environment/ecsdemo-crystal/kubernetes/service.yaml ~/environment/eksdemo/templates/service/crystal.yaml

# Copy and rename nodejs manifests
cp ~/environment/ecsdemo-nodejs/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/nodejs.yaml
cp ~/environment/ecsdemo-nodejs/kubernetes/service.yaml ~/environment/eksdemo/templates/service/nodejs.yaml

## Replace hard-coded values with template directives

The following steps should be completed seperately for frontend.yaml, crystal.yaml, and nodejs.yaml.

# DEPLOY THE EKSDEMO CHART

## Use the dry-run flag to test our templates
```
helm install --debug --dry-run workshop ~/environment/eksdemo

kubectl get svc ecsdemo-frontend -o jsonpath="{.status.loadBalancer.ingress[*].hostname}"; echo
```

helm history workshop
## Rollback the failed upgrade
```
# rollback to the 1st revision
helm rollback workshop 1

Rollback was a success! Happy Helming!
```

### error is gone!
helm uninstall workshop



# 3 week in AWS Health  checks
Kubernetes will restart a container if it crashes for any reason. It uses Liveness and Readiness probes which can be configured for running a robust application by identifying the healthy containers to send traffic to and restarting the ones when required.

## Configure the Probe
```
cat <<EoF > ~/environment/healthchecks/liveness-app.yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-app
spec:
  containers:
  - name: liveness
    image: brentley/ecsdemo-nodejs
    livenessProbe:
      httpGet:
        path: /health
        port: 3000
      initialDelaySeconds: 5
      periodSeconds: 5
EoF
```

kubectl apply -f ~/environment/healthchecks/liveness-app.yaml

### The kubectl describe command will show an event history which will show any probe failures or restarts.
```
andres-leonRangel:~/environment $ kubectl describe pod liveness-app
Name:         liveness-app
Namespace:    default
Priority:     0
Node:         ip-192-168-1-20.us-west-2.compute.internal/192.168.1.20
Start Time:   Wed, 07 Oct 2020 07:49:40 +0000
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"liveness-app","namespace":"default"},"spec":{"containers":[{"image":"...
              kubernetes.io/psp: eks.privileged
Status:       Running
IP:           192.168.16.248
IPs:
  IP:  192.168.16.248
Containers:
  liveness:
    Container ID:   docker://3c8957bd9eb65bf89e3f1fa9433e05fe68e47c8d023b22b813678cc370b27c21
    Image:          brentley/ecsdemo-nodejs
    Image ID:       docker-pullable://brentley/ecsdemo-nodejs@sha256:80169aea9f003c4425d34b0d079882f577feefcd044b955bd2a80ae316778dd9
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 07 Oct 2020 07:49:42 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:3000/health delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-p4cdp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-p4cdp:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-p4cdp
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From                                                 Message
  ----    ------     ----  ----                                                 -------
  Normal  Scheduled  112s  default-scheduler                                    Successfully assigned default/liveness-app to ip-192-168-1-20.us-west-2.compute.internal
  Normal  Pulling    111s  kubelet, ip-192-168-1-20.us-west-2.compute.internal  Pulling image "brentley/ecsdemo-nodejs"
  Normal  Pulled     110s  kubelet, ip-192-168-1-20.us-west-2.compute.internal  Successfully pulled image "brentley/ecsdemo-nodejs"
  Normal  Created    110s  kubelet, ip-192-168-1-20.us-west-2.compute.internal  Created container liveness
  Normal  Started    110s  kubelet, ip-192-168-1-20.us-west-2.compute.internal  Started container liveness
```

## https://www.eksworkshop.com/beginner/070_healthchecks/livenessprobe/#introduce-a-failure
send a SIGUSR1 signal to the nodejs application. By issuing this command we will send a kill signal to the application process in the docker runtime.

```
kubectl exec -it liveness-app -- /bin/kill -s SIGUSR1 1
andres-leonRangel:~/environment $ kubectl describe pod liveness-app
Name:         liveness-app
Namespace:    default
Priority:     0
Node:         ip-192-168-1-20.us-west-2.compute.internal/192.168.1.20
Start Time:   Wed, 07 Oct 2020 07:49:40 +0000
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"liveness-app","namespace":"default"},"spec":{"containers":[{"image":"...
              kubernetes.io/psp: eks.privileged
Status:       Running
IP:           192.168.16.248
IPs:
  IP:  192.168.16.248
Containers:
  liveness:
    Container ID:   docker://da927307208a49ae627a93afb022e6d17bef01ea51d193e97525308e6cc367ec
    Image:          brentley/ecsdemo-nodejs
    Image ID:       docker-pullable://brentley/ecsdemo-nodejs@sha256:80169aea9f003c4425d34b0d079882f577feefcd044b955bd2a80ae316778dd9
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 07 Oct 2020 07:54:12 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Wed, 07 Oct 2020 07:49:42 +0000
      Finished:     Wed, 07 Oct 2020 07:54:11 +0000
    Ready:          True
    Restart Count:  1
    Liveness:       http-get http://:3000/health delay=5s timeout=1s period=5s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-p4cdp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-p4cdp:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-p4cdp
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                  From                                                 Message
  ----     ------     ----                 ----                                                 -------
  Normal   Scheduled  4m43s                default-scheduler                                    Successfully assigned default/liveness-app to ip-192-168-1-20.us-west-2.compute.internal
  Warning  Unhealthy  42s (x3 over 52s)    kubelet, ip-192-168-1-20.us-west-2.compute.internal  Liveness probe failed: Get http://192.168.16.248:3000/health: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Normal   Killing    42s                  kubelet, ip-192-168-1-20.us-west-2.compute.internal  Container liveness failed liveness probe, will be restarted
  Normal   Pulling    12s (x2 over 4m42s)  kubelet, ip-192-168-1-20.us-west-2.compute.internal  Pulling image "brentley/ecsdemo-nodejs"
  Normal   Pulled     11s (x2 over 4m41s)  kubelet, ip-192-168-1-20.us-west-2.compute.internal  Successfully pulled image "brentley/ecsdemo-nodejs"
  Normal   Created    11s (x2 over 4m41s)  kubelet, ip-192-168-1-20.us-west-2.compute.internal  Created container liveness
  Normal   Started    11s (x2 over 4m41s)  kubelet, ip-192-168-1-20.us-west-2.compute.internal  Started container liveness
  ```

### status of container health check
`kubectl logs liveness-app`
`kubectl exec -it liveness-app -- curl -l http://liveness-app:3000/health`

#### You can also use kubectl logs to retrieve logs from a previous instantiation of a container with --previous flag, in case the container has crashed
kubectl logs liveness-app --previous

## Configure the Probe
readinessProbe definition explains how a linux command can be configured as healthcheck. 
```
command: ["sh", "-c", "touch /tmp/healthy && sleep 86400"]
        readinessProbe:
          exec:
            command:
            - cat
            - /tmp/healthy
```
## We will now create a deployment to test readiness probe
kubectl apply -f ~/environment/healthchecks/readiness-deployment.yaml

### Introduce a Failure
issue a command as below to delete the /tmp/healthy file which makes the readiness probe fail.
kubectl exec -it <YOUR-READINESS-POD-NAME> -- rm /tmp/healthy

Traffic will not be routed to the NOT READY pod in the above deployment.
`kubectl describe deployment readiness-deployment | grep Replicas:`


# IMPLEMENT AUTOSCALING WITH HPA AND CA
**Horizontal Pod Autoscaler (HPA)** scales the pods in a deployment or replica set. It is implemented as a K8s API resource and a controller. The controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. It obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).

*Cluster Autoscaler (CA)* a component that automatically adjusts the size of a Kubernetes Cluster so that all pods have a place to run and there are no unneeded nodes.

## Deploy the Metrics Server
deploy the metrics-server
andres-leonRangel:~/environment $ helm install metrics-server \
>     stable/metrics-server \
>     --version 2.11.1 \
>     --namespace metrics
NAME: metrics-server
LAST DEPLOYED: Wed Oct  7 08:23:03 2020
NAMESPACE: metrics
STATUS: deployed
REVISION: 1
NOTES:
The metric server has been deployed. 

In a few minutes you should be able to list metrics using the following
command:

  kubectl get --raw "/apis/metrics.k8s.io/v1beta1/nodes"

helm list --namespace=metrics

## Lets’ verify the status of the metrics-server APIService (it could take several minutes)
kubectl get apiservice v1beta1.metrics.k8s.io -o yaml | yq - r 'status'

