# 2020-10-08 Day 3 of AWS EKS workshop
Use [HPA to scale an App](https://www.eksworkshop.com/beginner/080_scaling/test_hpa/)

```
kubectl create deployment php-apache --image=us.gcr.io/k8s-artifacts-prod/hpa-example
kubectl set resources deploy php-apache --requests=cpu=200m
kubectl expose deploy php-apache --port 80

kubectl get pod -l app=php-apache
```

## Create an HPA resource
```
kubectl autoscale deployment php-apache `#The target average CPU utilization` \
    --cpu-percent=50 \
    --min=1 `#The lower limit for the number of pods that can be set by the autoscaler` \
    --max=10 `#The upper limit for the number of pods that can be set by the autoscaler`

kubectl get hpa
```

## Generate load to trigger scaling
```
kubectl --generator=run-pod/v1 run -i --tty load-generator --image=busybox /bin/sh
```

### HPA increases number of replicas to manage the load
```
NAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   486%/50%   1         10        1          4m46s



php-apache   Deployment/php-apache   486%/50%   1         10        4          4m49s
php-apache   Deployment/php-apache   486%/50%   1         10        8          5m4s
php-apache   Deployment/php-apache   486%/50%   1         10        10         5m19s
php-apache   Deployment/php-apache   51%/50%    1         10        10         5m35s
```

### It auto-scales down
```
NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   0%/50%    1         10        10         9m7s
php-apache   Deployment/php-apache   0%/50%    1         10        10         12m
php-apache   Deployment/php-apache   0%/50%    1         10        1          12m
```

## CONFIGURE CLUSTER AUTOSCALER (CA)
Cluster Autoscaler for AWS provides integration with Auto Scaling groups. It enables users to choose from four different options of deployment:

1. One Auto Scaling group
2. Multiple Auto Scaling groups
3. Auto-Discovery
4. Control-plane Node setup

### Configure AWS ASG auto-scaling group
aws autoscaling \
    describe-auto-scaling-groups \
    --query "AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') && Value=='eksworkshop-eksctl']].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]" \
    --output table
### increase the maximum capacity to 4 instances
```
# we need the ASG name
export ASG_NAME=$(aws autoscaling describe-auto-scaling-groups --query "AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') && Value=='eksworkshop-eksctl']].AutoScalingGroupName" --output text)

# increase max capacity up to 4
aws autoscaling \
    update-auto-scaling-group \
    --auto-scaling-group-name ${ASG_NAME} \
    # this params to the dirty job
    --min-size 3 \
    --desired-capacity 3 \
    --max-size 4

# Check new values
aws autoscaling \
    describe-auto-scaling-groups \
    --query "AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') && Value=='eksworkshop-eksctl']].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]" \
    --output table

```


## Enable AWS IAM service role
With IAM roles for service accounts on Amazon EKS clusters, you can associate an IAM role with a Kubernetes service account. This service account can then provide AWS permissions to the containers in any pod that uses that service account. With this feature, you no longer need to provide extended permissions to the node IAM role so that pods on that node can call AWS APIs.

Enabling IAM roles for service accounts on your cluster

```
eksctl utils associate-iam-oidc-provider \
>     --cluster eksworkshop-eksctl \
>     --approve
[ℹ]  eksctl version 0.29.1
[ℹ]  using region us-west-2
[ℹ]  will create IAM Open ID Connect provider for cluster "eksworkshop-eksctl" in "us-west-2"
[✔]  created IAM Open ID Connect provider for cluster "eksworkshop-eksctl" in "us-west-2"
```

### crete policy
```
andres-leonRangel:~/environment $ aws iam create-policy   \
>   --policy-name k8s-asg-policy \
>   --policy-document file://~/environment/cluster-autoscaler/k8s-asg-policy.json
{
    "Policy": {
        "PolicyName": "k8s-asg-policy",
        "PolicyId": "ANPARS5EXU3ANSJ5J6FWU",
        "Arn": "arn:aws:iam::109330278080:policy/k8s-asg-policy",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2020-10-07T21:47:12Z",
        "UpdateDate": "2020-10-07T21:47:12Z"
    }
}
```

### Create AWS IAM Role
```
eksctl create iamserviceaccount \
    --name cluster-autoscaler \
    --namespace kube-system \
    --cluster eksworkshop-eksctl \
    --attach-policy-arn "arn:aws:iam::${ACCOUNT_ID}:policy/k8s-asg-policy" \
    --approve \
    --override-existing-serviceaccounts

[ℹ]  eksctl version 0.29.1
[ℹ]  using region us-west-2
[ℹ]  2 iamserviceaccounts (kube-system/aws-node, kube-system/cluster-autoscaler) were included (based on the include/exclude rules)
[!]  metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set
[ℹ]  2 parallel tasks: { 2 sequential sub-tasks: { create IAM role for serviceaccount "kube-system/cluster-autoscaler", create serviceaccount "kube-system/cluster-autoscaler" }, 2 sequential sub-tasks: { create IAM role for serviceaccount "kube-system/aws-node", create serviceaccount "kube-system/aws-node" } }
[ℹ]  building iamserviceaccount stack "eksctl-eksworkshop-eksctl-addon-iamserviceaccount-kube-system-aws-node"
[ℹ]  building iamserviceaccount stack "eksctl-eksworkshop-eksctl-addon-iamserviceaccount-kube-system-cluster-autoscaler"
[ℹ]  deploying stack "eksctl-eksworkshop-eksctl-addon-iamserviceaccount-kube-system-aws-node"
[ℹ]  deploying stack "eksctl-eksworkshop-eksctl-addon-iamserviceaccount-kube-system-cluster-autoscaler"
[ℹ]  created serviceaccount "kube-system/cluster-autoscaler"
[ℹ]  serviceaccount "kube-system/aws-node" already exists
[ℹ]  updated serviceaccount "kube-system/aws-node"
```

### Verify service account and role association
```
kubectl -n kube-system describe sa cluster-autoscaler
Name:                cluster-autoscaler
Namespace:           kube-system
Labels:              <none>
Annotations:         eks.amazonaws.com/role-arn: arn:aws:iam::109330278080:role/eksctl-eksworkshop-eksctl-addon-iamserviceac-Role1-43Y5256UNY4M
Image pull secrets:  <none>
Mountable secrets:   cluster-autoscaler-token-8zvw6
Tokens:              cluster-autoscaler-token-8zvw6
Events:              <none>
```

## Deploy [Autoscaler](https://www.eksworkshop.com/beginner/080_scaling/deploy_ca/#deploy-the-cluster-autoscaler-ca)
```
clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created
role.rbac.authorization.k8s.io/cluster-autoscaler created
clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
deployment.apps/cluster-autoscaler created
```

## Add kubernetes annotation properties

To prevent CA from removing nodes where its own pod is running, we will add the cluster-autoscaler.kubernetes.io/safe-to-evict annotation to its deployment with the following command

```
kubectl -n kube-system \
    annotate deployment.apps/cluster-autoscaler \
    cluster-autoscaler.kubernetes.io/safe-to-evict="false"
```

## update the autoscaler image
```
# we need to retrieve the latest docker image available for our EKS version
export K8S_VERSION=$(kubectl version --short | grep 'Server Version:' | sed 's/[^0-9.]*\([0-9.]*\).*/\1/' | cut -d. -f1,2)
export AUTOSCALER_VERSION=$(curl -s "https://api.github.com/repos/kubernetes/autoscaler/releases" | grep '"tag_name":' | sed -s 's/.*-\([0-9][0-9\.]*\).*/\1/' | grep -m1 ${K8S_VERSION})

kubectl -n kube-system \
    set image deployment.apps/cluster-autoscaler \
    cluster-autoscaler=us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v${AUTOSCALER_VERSION}

>     set image deployment.apps/cluster-autoscaler \
>     cluster-autoscaler=us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v${AUTOSCALER_VERSION}
deployment.apps/cluster-autoscaler image updated
```

# Scale a cluster with CA
- create an app that requires lots of load
- the auto scaler will start a new EC2 instance and distribute load

## scale
`kubectl scale --replicas=10 deployment/nginx-to-scaleout`kubectl get pods -l app=nginx -o wide --watch

## Clean up
ASG
```
aws autoscaling \
  update-auto-scaling-group \
  --auto-scaling-group-name ${ASG_NAME} \
  --min-size 1 \
  --desired-capacity 1 \
  --max-size 3
```

## Result in AWS to save money
```
andres-leonRangel:~/environment $ kubectl get nodes
NAME                                           STATUS                     ROLES    AGE     VERSION
ip-192-168-1-20.us-west-2.compute.internal     Ready,SchedulingDisabled   <none>   37h     v1.17.11-eks-cfdc40
ip-192-168-41-165.us-west-2.compute.internal   Ready                      <none>   4m43s   v1.17.11-eks-cfdc40
ip-192-168-48-149.us-west-2.compute.internal   Ready,SchedulingDisabled   <none>   37h     v1.17.11-eks-cfdc40
ip-192-168-95-54.us-west-2.compute.internal    Ready,SchedulingDisabled   <none>   37h     v1.17.11-eks-cfdc40
andres-leonRangel:~/environment $ kubectl get nodes
NAME                                           STATUS                        ROLES    AGE     VERSION
ip-192-168-1-20.us-west-2.compute.internal     NotReady,SchedulingDisabled   <none>   37h     v1.17.11-eks-cfdc40
ip-192-168-41-165.us-west-2.compute.internal   Ready 
```

# 6 INTRO TO RBAC
AWS Page [Intro to RBAC](https://www.eksworkshop.com/beginner/090_rbac/intro/)