# 2020-10-09-EKS Workshop
## CONNECTING APPLICATIONS WITH SERVICES
containers within a Pod can all reach each other’s ports on localhost, and all pods in a cluster can see each other without NA

### Check if default-deny.yaml exists and delete it
``` bash
if [ -f ~/environment/calico_resources/default-deny.yaml ]; then
  kubectl delete -f ~/environment/calico_resources/default-deny.yaml
fi
```

## Get pods IP
kubectl -n my-nginx get pods -o yaml | grep 'podIP:'

what happens when a node dies? The pods die with it, and the Deployment will create new ones, with different IPs.

When created, each Service is assigned a unique IP address (also called clusterIP). This address is tied to the lifespan of the Service, and will not change while the Service is alive. Pods can be configured to talk to the Service, and know that communication to the Service will be automatically load-balanced out to some pod that is a member of the Service.

### create a Service for your 2 nginx replicas with kubectl expose
`kubectl -n my-nginx expose deployment/my-nginx`

a Service is backed by a group of Pods. These Pods are exposed through endpoints. The Service’s selector will be evaluated continuously and the results will be POSTed to an Endpoints object also named my-nginx. When a Pod dies, it is automatically removed from the endpoints, and new Pods matching the Service’s selector will automatically get added to the endpoints. Check the endpoints, and note that the IPs are the same as the Pods created in the first step:
`kubectl -n my-nginx describe svc my-nginx`

The Service IP is completely virtual, it never hits the wire.

`# Create a variable set with the my-nginx service IP
export MyClusterIP=$(kubectl -n my-nginx get svc my-nginx -ojsonpath='{.spec.clusterIP}')
`

### get some meta-data THROUGH ENV VARS
export mypod=$(kubectl -n my-nginx get pods  -o jsonpath='{.items[0].metadata.name}')

kubectl -n my-nginx exec ${mypod} -- printenv | grep SERVICE

### restart deployment
`kubectl -n my-nginx rollout restart deployment my-nginx`

### Use DNS to get details about services (requires CoreDNS)
kubectl get pod -n kube-system -l k8s-app=kube-dns
`kubectl run curly_pod --generator=run-pod/v1 --image=radial/busyboxplus:curl -i --tty`

## EXPOSING THE SERVICE
For some parts of your applications you may want to expose a Service onto an external IP address. Kubernetes supports two ways of doing this: NodePort and LoadBalancer.

Currently the Service does not have an External IP, so let’s now patch the Service to use a cloud load balancer, by updating the type of the my-nginx Service from ClusterIP to LoadBalancer

`kubectl -n my-nginx patch svc my-nginx -p '{"spec": {"type": "LoadBalancer"}}'`

### Access a K8s Load balancer 
```
export loadbalancer=$(kubectl -n my-nginx get svc my-nginx -o jsonpath='{.status.loadBalancer.ingress[*].hostname}')

curl -k -s http://${loadbalancer} | grep title
```

### find the ingress of the service
`kubectl -n my-nginx describe service my-nginx | grep Ingress` 

## INGRESS
Ingress, added in Kubernetes v1.1, exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.
Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer

### The Ingress Resource
A minimal ingress resource example for ingress-nginx:
Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which is the rewrite-target annotation. Different Ingress controller support different annotations. Review the documentation for your choice of Ingress controller to learn which annotations are supported.

Ingress Controllers
In order for the Ingress resource to work, the cluster must have an ingress controller running.

Unlike other types of controllers which run as part of the kube-controller-manager binary, Ingress controllers are not started automatically with a cluster.

# ASSIGNING PODS TO NODES
the strategy of assigning Pods works, alternatives and recommended approaches.

You can constrain a pod to only be able to run on particular nodes or to prefer to run on particular nodes.

Generally such constraints are unnecessary, as the scheduler will automatically do a reasonable placement (e.g. spread your pods across nodes, not place the pod on a node with insufficient free resources, etc.) but there are some circumstances where you may want more control on a node where a pod lands, e.g. to ensure that a pod ends up on a machine with an SSD attached to it, or to co-locate pods from two different services that communicate a lot into the same availability zone.

## [NODESELECTOR](https://www.eksworkshop.com/beginner/140_assigning_pods/node_selector/)
10 modules in begginer to go
All intermediate
all advanced
